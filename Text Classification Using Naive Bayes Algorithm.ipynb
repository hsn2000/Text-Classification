{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8fa3b61",
   "metadata": {},
   "source": [
    "# All words from docs represented as tf-idf vectors used as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e96d673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import pathlib\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa350c1",
   "metadata": {},
   "source": [
    "# Reading Filenames and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12322644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of course docs:  230\n",
      "Total number of non-course docs:  821\n",
      "Total docs:  1051\n"
     ]
    }
   ],
   "source": [
    "directory=os.getcwd()+'\\\\fulltext'+'\\course'\n",
    "# print(directory)\n",
    "data={}\n",
    "for text_file in os.listdir(directory):\n",
    "    data[text_file]=1\n",
    "print(\"Total number of course docs: \",len(os.listdir(directory)))\n",
    "\n",
    "directory=os.getcwd()+'\\\\fulltext'+'\\\\non-course'\n",
    "# print(directory)\n",
    "for text_file in os.listdir(directory):\n",
    "    data[text_file]=0\n",
    "print(\"Total number of non-course docs: \",len(os.listdir(directory)))\n",
    "print(\"Total docs: \",len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74ece08c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'http_^^cs.cornell.edu^Info^Courses^Current^CS415^CS414.html': 1,\n",
       " 'http_^^cs.cornell.edu^Info^Courses^Fall-95^CS415^CS415.html': 1,\n",
       " 'http_^^cs.cornell.edu^Info^Courses^Spring-96^CS432^cs432.html': 1,\n",
       " 'http_^^simon.cs.cornell.edu^Info^Courses^Current^CS401^': 1,\n",
       " 'http_^^simon.cs.cornell.edu^Info^Courses^Spring-96^CS515^': 1}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: data[k] for k in list(data)[:5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f5eb705",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_titles = []\n",
    "doc_labels = []\n",
    "for key,value in data.items():\n",
    "    doc_titles.append(key)\n",
    "    doc_labels.append(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893530f0",
   "metadata": {},
   "source": [
    "# Splitting Dataset into 85:15 ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b47e905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of Training Data : 893\n",
      "Total number of Test Data : 158\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(doc_titles,doc_labels,test_size=0.15,stratify=doc_labels,shuffle=True)\n",
    "print ('Total number of Training Data :',len(xtrain))\n",
    "print ('Total number of Test Data :',len(xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d22807a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http_^^www.cs.wisc.edu^~krung^krung.html',\n",
       " 'http_^^www.cs.wisc.edu^~milo^milo.html',\n",
       " 'http_^^www.cs.washington.edu^education^courses^cse567',\n",
       " 'http_^^www.cs.cornell.edu^Info^People^scl^sean.html',\n",
       " 'http_^^www.cs.utexas.edu^users^emery^']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75a1224e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 0, 0]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytrain[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "874aed8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of Training Data : 893\n",
      "Total number of Test Data : 158\n"
     ]
    }
   ],
   "source": [
    "print ('Total number of Training Data :',len(xtrain))\n",
    "print ('Total number of Test Data :',len(xtest))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ce3e49",
   "metadata": {},
   "source": [
    "# Reading Documents data and cleaning it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb40e5e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Hasan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Hasan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Hasan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords\n",
    "stopword = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ebfce2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pre_Processing(word):\n",
    "    word=case_folding(word)\n",
    "    word=remove_punctuation(word)\n",
    "    word=lemmatization(word)\n",
    "    return(word)\n",
    "\n",
    "def lemmatization(wor):\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    return(lemmatizer.lemmatize(wor))\n",
    "\n",
    "def case_folding(wo):\n",
    "    return(wo.lower())\n",
    "\n",
    "def remove_punctuation(w):  \n",
    "    import re\n",
    "    w=re.sub('(\\d)',\"\",w)\n",
    "    w=w.replace(\"—\",\"\")\n",
    "    w=w.replace(\"_\",\"\")\n",
    "    w=w.replace(\"?\",\"\")\n",
    "    w=w.replace(\".\",\"\")\n",
    "    w=w.replace(\"`\",\"\")\n",
    "    w=w.replace(\",\",\"\")\n",
    "    w=w.replace(\"[\",\"\")\n",
    "    w=w.replace(\"]\",\"\")\n",
    "    w=w.replace(\"â€”\",\"\")\n",
    "    w=w.replace(\":\",\"\")\n",
    "    w=w.replace(\"|\",\"\")\n",
    "    w=w.replace(\";\",\"\")\n",
    "    w=w.replace(\"{\",\"\")\n",
    "    w=w.replace(\"}\",\"\")\n",
    "    w=w.replace(\"-\",\"\")\n",
    "    w=w.replace(\"=\",\"\")\n",
    "    w=w.replace(\"…\",\"\")\n",
    "    w=w.replace(\"Â\",\"\")\n",
    "    w=w.replace(\"/\",\"\")\n",
    "    w=w.replace(\"â\",\"\")\n",
    "    w=w.replace(\"'\",\"\")\n",
    "    w=w.replace(\"–\",\"\")\n",
    "    w=w.replace('\"',\"\")\n",
    "    w=w.replace(\"$\",\"\")\n",
    "    w=w.replace(\"â–\",\"\")\n",
    "    w=w.replace(\"%\",\"\")\n",
    "    w=w.replace(\"(\",\"\")\n",
    "    w=w.replace(\"&\",\"\")\n",
    "    w=w.replace(\")\",\"\")\n",
    "    w=w.replace(\"ã©\",\"\")\n",
    "    w=w.replace(\"!\",\"\")\n",
    "    return(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "639f0b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_doc_data(doc_name,doc_label,corpus):\n",
    "    directory=os.getcwd()\n",
    "    i=0\n",
    "    doc_data={}\n",
    "    for doc,label in zip(doc_name,doc_label):\n",
    "        wordlist=[]\n",
    "        dir1=directory\n",
    "        if label==1:\n",
    "            dir1=directory+\"\\\\fulltext\\\\course\"\n",
    "        if label==0:\n",
    "            dir1=directory+\"\\\\fulltext\\\\non-course\"\n",
    "        f=open(dir1+\"\\\\\"+doc,'r')\n",
    "        text=f.read()\n",
    "        soup = BeautifulSoup(text,features=\"lxml\")\n",
    "        lines=soup.get_text()\n",
    "        lines=lines.replace(\"\\n\",\" \")\n",
    "        lines=lines.replace(\"(\",\" \")\n",
    "        lines=lines.replace(\")\",\" \")\n",
    "        lines=lines.replace(\":\",\" \")\n",
    "        lines=lines.replace(\",\",\" \")\n",
    "        iii=\"  \"\n",
    "        for iii in lines:\n",
    "            lines=lines.replace(\"  \",\" \")\n",
    "\n",
    "        wordlist=lines.split(\" \")\n",
    "        wordlist2=[]\n",
    "        for word in wordlist:\n",
    "            if word not in stopword:\n",
    "                word=Pre_Processing(word)\n",
    "                if word=='' or word==' 'or word=='  ':\n",
    "                    continue\n",
    "                word=word.strip()\n",
    "                if len(word)<=2:\n",
    "                    continue\n",
    "                wordlist2.append(word)\n",
    "                if word not in corpus:\n",
    "                    corpus.append(word)\n",
    "        doc_data[doc]=wordlist2 \n",
    "    return doc_data,corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89d9c422",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_data, corpus = read_doc_data(xtrain,ytrain,[])\n",
    "xtest_data, corpus = read_doc_data(xtest,ytest,corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8d3a995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'http_^^www.cs.wisc.edu^~krung^krung.html': ['krungs',\n",
       "  'homepage',\n",
       "  'updated',\n",
       "  'november',\n",
       "  'krungs',\n",
       "  'homepage',\n",
       "  'underconstruction',\n",
       "  'try',\n",
       "  'keep',\n",
       "  'page',\n",
       "  'short',\n",
       "  'informative',\n",
       "  'have',\n",
       "  'good',\n",
       "  'serf',\n",
       "  'the',\n",
       "  'year',\n",
       "  'come',\n",
       "  'the',\n",
       "  'following',\n",
       "  'web',\n",
       "  'related',\n",
       "  'topic',\n",
       "  'research',\n",
       "  'mathematical',\n",
       "  'programming',\n",
       "  'project',\n",
       "  'pursuing',\n",
       "  'course',\n",
       "  'work',\n",
       "  'old',\n",
       "  'course',\n",
       "  'work',\n",
       "  'computer',\n",
       "  'science',\n",
       "  'department',\n",
       "  'computer',\n",
       "  'company',\n",
       "  'favorite',\n",
       "  'hobby',\n",
       "  'personal',\n",
       "  'information',\n",
       "  'personal',\n",
       "  'opinion',\n",
       "  'life',\n",
       "  'madisonwisconsin',\n",
       "  'linked',\n",
       "  'the',\n",
       "  'following',\n",
       "  'web',\n",
       "  'page',\n",
       "  'important',\n",
       "  'link',\n",
       "  'university',\n",
       "  'madisonwisconsin',\n",
       "  'whole',\n",
       "  'computer',\n",
       "  'science',\n",
       "  'department',\n",
       "  'unique',\n",
       "  'entity',\n",
       "  'electronic',\n",
       "  'library',\n",
       "  'system',\n",
       "  'krung',\n",
       "  'sinapiromsaran',\n",
       "  'email',\n",
       "  'krung@cswiscedu'],\n",
       " 'http_^^www.cs.wisc.edu^~milo^milo.html': ['milo',\n",
       "  'martin',\n",
       "  'home',\n",
       "  'page',\n",
       "  'milo',\n",
       "  'martin',\n",
       "  'milo@cswiscedu',\n",
       "  'graduate',\n",
       "  'student',\n",
       "  'teaching',\n",
       "  'assistant',\n",
       "  'computer',\n",
       "  'science',\n",
       "  'department',\n",
       "  'university',\n",
       "  'wisconsinmadison',\n",
       "  'west',\n",
       "  'dayton',\n",
       "  'street',\n",
       "  'madison',\n",
       "  'usa',\n",
       "  'email',\n",
       "  'milo@cswiscedu',\n",
       "  'office',\n",
       "  'csst',\n",
       "  'office',\n",
       "  'phone',\n",
       "  'office',\n",
       "  'hour',\n",
       "  'tuesdaythursday',\n",
       "  'appointment',\n",
       "  'computer',\n",
       "  'science',\n",
       "  'gustavus',\n",
       "  'adolphus',\n",
       "  'college',\n",
       "  'class',\n",
       "  'compiler',\n",
       "  'construction',\n",
       "  'charles',\n",
       "  'fischer',\n",
       "  'advanced',\n",
       "  'computer',\n",
       "  'architecture',\n",
       "  'mark',\n",
       "  'hill',\n",
       "  'java',\n",
       "  'sitting',\n",
       "  'mark',\n",
       "  'hill',\n",
       "  'james',\n",
       "  'larus',\n",
       "  'teaching',\n",
       "  'algebraic',\n",
       "  'language',\n",
       "  'programming',\n",
       "  'c++',\n",
       "  'section',\n",
       "  'research',\n",
       "  'interest',\n",
       "  'first',\n",
       "  'year',\n",
       "  'phd',\n",
       "  'student',\n",
       "  'interested',\n",
       "  'programming',\n",
       "  'language',\n",
       "  'architecture',\n",
       "  'system',\n",
       "  'specifically',\n",
       "  'interested',\n",
       "  'compiler',\n",
       "  'optimization',\n",
       "  'technology',\n",
       "  'influenced',\n",
       "  'hardware',\n",
       "  'operating',\n",
       "  'system',\n",
       "  'advance',\n",
       "  'mobile',\n",
       "  'programming',\n",
       "  'java',\n",
       "  'additional',\n",
       "  'challenge',\n",
       "  'present',\n",
       "  'compiler',\n",
       "  'architecture',\n",
       "  'operating',\n",
       "  'system',\n",
       "  'designer',\n",
       "  'many',\n",
       "  'many',\n",
       "  'thing',\n",
       "  'many',\n",
       "  'even',\n",
       "  'know',\n",
       "  'interested',\n",
       "  'yet',\n",
       "  'publication',\n",
       "  'research',\n",
       "  'performed',\n",
       "  'summer',\n",
       "  'argonne',\n",
       "  'national',\n",
       "  'laboratory',\n",
       "  'technology',\n",
       "  'development',\n",
       "  'division',\n",
       "  'advisement',\n",
       "  'charles',\n",
       "  'fink',\n",
       "  'fink',\n",
       "  'humm',\n",
       "  'martin',\n",
       "  'micklich',\n",
       "  'evaluation',\n",
       "  'fewview',\n",
       "  'reconstruction',\n",
       "  'parameter',\n",
       "  'illicit',\n",
       "  'substance',\n",
       "  'detection',\n",
       "  'using',\n",
       "  'fastneutron',\n",
       "  'transmission',\n",
       "  'spectroscopy',\n",
       "  'ieee',\n",
       "  'nuclear',\n",
       "  'science',\n",
       "  'symposium',\n",
       "  'medical',\n",
       "  'imaging',\n",
       "  'conference',\n",
       "  'fink',\n",
       "  'micklich',\n",
       "  'yule',\n",
       "  'humm',\n",
       "  'sagalovsky',\n",
       "  'martin',\n",
       "  'evaluation',\n",
       "  'neutron',\n",
       "  'technique',\n",
       "  'illicit',\n",
       "  'substance',\n",
       "  'detection',\n",
       "  'nucl',\n",
       "  'inst',\n",
       "  'meth',\n",
       "  'unpublications',\n",
       "  'research',\n",
       "  'performed',\n",
       "  'school',\n",
       "  'year',\n",
       "  'gustavus',\n",
       "  'adolphus',\n",
       "  'college',\n",
       "  'advisement',\n",
       "  'max',\n",
       "  'hailperin',\n",
       "  'milo',\n",
       "  'martin',\n",
       "  'max',\n",
       "  'hailperin',\n",
       "  'programming',\n",
       "  'language',\n",
       "  'flexibility',\n",
       "  'deterministic',\n",
       "  'dynamic',\n",
       "  'parallel',\n",
       "  'computation',\n",
       "  'senior',\n",
       "  'honor',\n",
       "  'thesis',\n",
       "  'mathematics',\n",
       "  'computer',\n",
       "  'science',\n",
       "  'department',\n",
       "  'gustavus',\n",
       "  'adolphus',\n",
       "  'college',\n",
       "  'dvi',\n",
       "  'postscript',\n",
       "  'computing',\n",
       "  'interest',\n",
       "  'java',\n",
       "  'other',\n",
       "  'java',\n",
       "  'resource',\n",
       "  'next',\n",
       "  'software',\n",
       "  'once',\n",
       "  'next',\n",
       "  'computer',\n",
       "  'software',\n",
       "  'company',\n",
       "  'acm',\n",
       "  'acm',\n",
       "  'founded',\n",
       "  'international',\n",
       "  'scientific',\n",
       "  'educational',\n",
       "  'organization',\n",
       "  'dedicated',\n",
       "  'advancing',\n",
       "  'art',\n",
       "  'science',\n",
       "  'engineering',\n",
       "  'application',\n",
       "  'information',\n",
       "  'technology',\n",
       "  'serving',\n",
       "  'professional',\n",
       "  'public',\n",
       "  'interest',\n",
       "  'fostering',\n",
       "  'open',\n",
       "  'interchange',\n",
       "  'information',\n",
       "  'promoting',\n",
       "  'highest',\n",
       "  'professional',\n",
       "  'ethical',\n",
       "  'standard',\n",
       "  'direct',\n",
       "  'quote',\n",
       "  'web',\n",
       "  'page',\n",
       "  'personal',\n",
       "  'interest',\n",
       "  'nfl',\n",
       "  'football',\n",
       "  'big',\n",
       "  'nfl',\n",
       "  'football',\n",
       "  'fan',\n",
       "  'since',\n",
       "  'lived',\n",
       "  'minnesota',\n",
       "  'year',\n",
       "  'favorite',\n",
       "  'team',\n",
       "  'minnesota',\n",
       "  'viking',\n",
       "  'even',\n",
       "  'though',\n",
       "  'live',\n",
       "  'land',\n",
       "  'cheese',\n",
       "  'head',\n",
       "  'colonize',\n",
       "  'conquer',\n",
       "  'multiplayer',\n",
       "  'playbyemail',\n",
       "  'space',\n",
       "  'exploration',\n",
       "  'combat',\n",
       "  'game',\n",
       "  'wrote',\n",
       "  'babylon',\n",
       "  'the',\n",
       "  'best',\n",
       "  'show',\n",
       "  'imho',\n",
       "  'atlantis',\n",
       "  'atlantis',\n",
       "  'playbyemail',\n",
       "  'game',\n",
       "  'set',\n",
       "  'mythical',\n",
       "  'world',\n",
       "  'atlantis',\n",
       "  'player',\n",
       "  'build',\n",
       "  'army',\n",
       "  'engauge',\n",
       "  'trade',\n",
       "  'explore',\n",
       "  'land',\n",
       "  'fight',\n",
       "  'wondering',\n",
       "  'monster',\n",
       "  'train',\n",
       "  'wizard',\n",
       "  'discover',\n",
       "  'underworld',\n",
       "  'right',\n",
       "  'player',\n",
       "  'one',\n",
       "  'rule',\n",
       "  'current',\n",
       "  'list',\n",
       "  'player',\n",
       "  'ultimate',\n",
       "  'frisbee',\n",
       "  'upa',\n",
       "  'the',\n",
       "  'ultimate',\n",
       "  'player',\n",
       "  'association',\n",
       "  'ultimate',\n",
       "  'combine',\n",
       "  'element',\n",
       "  'soccer',\n",
       "  'football',\n",
       "  'basketball',\n",
       "  'fastpaced',\n",
       "  'game',\n",
       "  'played',\n",
       "  'frisbee',\n",
       "  'everyone',\n",
       "  'quarterback',\n",
       "  'everyone',\n",
       "  'receiver',\n",
       "  'direct',\n",
       "  'quote',\n",
       "  'upa',\n",
       "  'home',\n",
       "  'page',\n",
       "  'ultimate',\n",
       "  'ten',\n",
       "  'simple',\n",
       "  'rule',\n",
       "  'milo',\n",
       "  'martin',\n",
       "  'milo@cswiscedu']}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: xtrain_data[k] for k in list(xtrain_data)[:2]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3632926",
   "metadata": {},
   "source": [
    "# Saving Data in files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f6cc13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_doc_data(data,filename):\n",
    "    f=open(filename,'w')\n",
    "    for doc in data:\n",
    "        f.write(doc)\n",
    "        f.write(\":\")\n",
    "        for word in data[doc]:\n",
    "            f.write(word)\n",
    "            f.write(\",\")\n",
    "        f.write(\"\\n\")\n",
    "    f.close()\n",
    "    \n",
    "def save_corpus():\n",
    "    f=open(\"corpus.txt\",'w')\n",
    "    for word in corpus:\n",
    "        f.write(word)\n",
    "        f.write(\"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3de055ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_doc_data(xtrain_data,\"train_doc_data.txt\")\n",
    "save_doc_data(xtest_data,\"test_doc_data.txt\")\n",
    "save_corpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619b60f5",
   "metadata": {},
   "source": [
    "# Reading Data from Saved files(if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38b40dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_from_file():\n",
    "    xtrain_data={}\n",
    "    xtest_data={}\n",
    "    f=open(\"train_doc_data.txt\",'r')\n",
    "    text=f.read()\n",
    "    text=text.split('\\n')\n",
    "    for line in text:\n",
    "        line=line.split(\":\")\n",
    "        line1=line[1]\n",
    "        line1=line1.removesuffix(\",\")\n",
    "        line1=line1.split(',')\n",
    "        xtrain_data[line[0]]=line1\n",
    "    f.close()\n",
    "    f=open(\"test_doc_data.txt\",'r')\n",
    "    text=f.read()\n",
    "    text=text.split('\\n')\n",
    "    for line in text:\n",
    "        line=line.split(\":\")\n",
    "        line1=line[1]\n",
    "        line1=line1.removesuffix(\",\")\n",
    "        line1=line1.split(',')\n",
    "        xtest_data[line[0]]=line1\n",
    "    f.close()\n",
    "    return xtrain_data, xtest_data\n",
    "\n",
    "def read_corpus():\n",
    "    corpus=[]\n",
    "    f=open(\"corpus.txt\",'r')\n",
    "    text=f.read()\n",
    "    text=text.split('\\n')\n",
    "    for word in text:\n",
    "        if word!='' or word!=' 'or word!='  ':\n",
    "            corpus.append(word)\n",
    "    f.close()\n",
    "    return corpus\n",
    "    \n",
    "# xtrain_data, xtest_data = read_data_from_file()\n",
    "# corpus = read_corpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689abc65",
   "metadata": {},
   "source": [
    "# Displaying Cleaned Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dbf6e32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_text=[]\n",
    "for doc in xtrain_data:\n",
    "    tmp=' '.join([item for item in xtrain_data[doc]])\n",
    "    xtrain_text.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5957166b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['krungs homepage updated november krungs homepage underconstruction try keep page short informative have good serf the year come the following web related topic research mathematical programming project pursuing course work old course work computer science department computer company favorite hobby personal information personal opinion life madisonwisconsin linked the following web page important link university madisonwisconsin whole computer science department unique entity electronic library system krung sinapiromsaran email krung@cswiscedu',\n",
       " 'milo martin home page milo martin milo@cswiscedu graduate student teaching assistant computer science department university wisconsinmadison west dayton street madison usa email milo@cswiscedu office csst office phone office hour tuesdaythursday appointment computer science gustavus adolphus college class compiler construction charles fischer advanced computer architecture mark hill java sitting mark hill james larus teaching algebraic language programming c++ section research interest first year phd student interested programming language architecture system specifically interested compiler optimization technology influenced hardware operating system advance mobile programming java additional challenge present compiler architecture operating system designer many many thing many even know interested yet publication research performed summer argonne national laboratory technology development division advisement charles fink fink humm martin micklich evaluation fewview reconstruction parameter illicit substance detection using fastneutron transmission spectroscopy ieee nuclear science symposium medical imaging conference fink micklich yule humm sagalovsky martin evaluation neutron technique illicit substance detection nucl inst meth unpublications research performed school year gustavus adolphus college advisement max hailperin milo martin max hailperin programming language flexibility deterministic dynamic parallel computation senior honor thesis mathematics computer science department gustavus adolphus college dvi postscript computing interest java other java resource next software once next computer software company acm acm founded international scientific educational organization dedicated advancing art science engineering application information technology serving professional public interest fostering open interchange information promoting highest professional ethical standard direct quote web page personal interest nfl football big nfl football fan since lived minnesota year favorite team minnesota viking even though live land cheese head colonize conquer multiplayer playbyemail space exploration combat game wrote babylon the best show imho atlantis atlantis playbyemail game set mythical world atlantis player build army engauge trade explore land fight wondering monster train wizard discover underworld right player one rule current list player ultimate frisbee upa the ultimate player association ultimate combine element soccer football basketball fastpaced game played frisbee everyone quarterback everyone receiver direct quote upa home page ultimate ten simple rule milo martin milo@cswiscedu']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_text[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9f1c2e",
   "metadata": {},
   "source": [
    "# Generating Doc index for faster access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26231da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Training\n",
    "train_doc_index={}\n",
    "for i in range(len(xtrain)):\n",
    "    train_doc_index[xtrain[i]]=i\n",
    "train_doc_index_invert={}\n",
    "for i in range(len(xtrain)):\n",
    "    train_doc_index_invert[i]=xtrain[i]\n",
    "    \n",
    "# For Testing\n",
    "test_doc_index={}\n",
    "for i in range(len(xtest)):\n",
    "    test_doc_index[xtest[i]]=i\n",
    "test_doc_index_invert={}\n",
    "for i in range(len(xtest)):\n",
    "    test_doc_index_invert[i]=xtest[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0433b7",
   "metadata": {},
   "source": [
    "# Method 1: Using all words in the document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdaba0b",
   "metadata": {},
   "source": [
    "# Generating a dictionary to know which word is coming in which document how many times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b838ae64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "893"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(xtrain_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513a405b",
   "metadata": {},
   "source": [
    "There are 893 docs in training data so for each word there will be a list of length 893. Each number from 0-892 will represent a doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "87f6c477",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_index(data): \n",
    "    Index={}\n",
    "    i=0\n",
    "    for doc in data:\n",
    "        for word in data[doc]:\n",
    "            if word not in Index:\n",
    "                Index[word]=[]\n",
    "                Index[word]=[0]*(len(data))\n",
    "            Index[word][i]=Index[word][i]+1\n",
    "        i=i+1\n",
    "    return Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "38068ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Training_Index = generate_index(xtrain_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f67d09a",
   "metadata": {},
   "source": [
    "# Calculating tf-idf scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e24e7f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_score(Index, data):\n",
    "    idf_scores={}\n",
    "    for word in Index:\n",
    "        df=0\n",
    "        for i in range(len(data)):\n",
    "            if int(Index[word][i])>0:\n",
    "                df+=1\n",
    "        idf=math.log10(len(data)/df)\n",
    "        idf_scores[word]=idf\n",
    "    return idf_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8bd3dc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_scores = tf_idf_score(Training_Index, xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7b98425a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'krungs': 2.9508514588885464,\n",
       " 'homepage': 0.9966089494492215,\n",
       " 'updated': 0.843641489240678}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: idf_scores[k] for k in list(idf_scores)[:3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ab5a23f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_docs(Index, idf_scores, data):\n",
    "    doc_dict={}\n",
    "    for i in range(len(data)):\n",
    "        D=[]\n",
    "        for word in Index:\n",
    "            if int(Index[word][i]) ==0 or float(idf_scores[word])==0:\n",
    "                D.append(0)\n",
    "                continue\n",
    "            D.append(int(Index[word][i])*float(idf_scores[word]))\n",
    "        doc_dict[i]=D\n",
    "    return doc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "996d2352",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tf_idf_docs_dict = tf_idf_docs(Training_Index, idf_scores, xtrain)\n",
    "# Since we'll only use the training index(words appear in training data) and ignore the words that \n",
    "# are not in training data and will directly convert testing docs to tf-idf vectors based on training data\n",
    "# test_tf_idf_docs_dict = tf_idf_docs(Training_Index, training_idf_scores, xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5a6c61",
   "metadata": {},
   "source": [
    "Documents are vectorized on the basis of term-frequency-inverse-doc-frequency scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd15e39e",
   "metadata": {},
   "source": [
    "# Preparing Data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8bd62220",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_doc_vectors(tf_idf_docs_dict):\n",
    "    doc_vectors=[]\n",
    "    i=0\n",
    "    for doc in tf_idf_docs_dict:\n",
    "        doc_vectors.append([])\n",
    "        doc_vectors[i]=tf_idf_docs_dict[doc]\n",
    "        i+=1\n",
    "    return doc_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a0bb331c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tf_idf_doc_vectors = to_doc_vectors(train_tf_idf_docs_dict)\n",
    "# test_tf_idf_doc_vectors = to_doc_vectors(test_tf_idf_docs_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a845c06c",
   "metadata": {},
   "source": [
    "Data is ready to be fed to the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92e4b29",
   "metadata": {},
   "source": [
    "# Importing Naive Bayes model and fitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "08ff4b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "clf = MultinomialNB().fit(train_tf_idf_doc_vectors,ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95949363",
   "metadata": {},
   "source": [
    "Data has been fitted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d60146",
   "metadata": {},
   "source": [
    "# Preparing Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0bf2e8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Testing_Index = generate_index(xtest_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4324b705",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index={}\n",
    "i=0\n",
    "for word in idf_scores:\n",
    "    word_index[word]=i\n",
    "    i=i+1\n",
    "test_tf_idf_doc_vectors=[]\n",
    "for doc in xtest_data:\n",
    "    D=[0]*(len(idf_scores))\n",
    "    for word in xtest_data[doc]:\n",
    "        if word in Training_Index:\n",
    "            D[word_index[word]]=int(Testing_Index[word][test_doc_index[doc]])*float(idf_scores[word])\n",
    "    test_tf_idf_doc_vectors.append(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9d89b8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = clf.predict(test_tf_idf_doc_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "879fc8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 0 0 0 0 0 0 1 0 1 1 0 1 1 1 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 1 1\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0\n",
      " 0 1 0 0 0 0 0 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e53c21d",
   "metadata": {},
   "source": [
    "# 1=course   0=Non-course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "95dbf882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Accuracy of the classifier is 0.930379746835443\n",
      "\n",
      " Confusion matrix\n",
      "[[117   6]\n",
      " [  5  30]]\n",
      "\n",
      " The value of Precision 0.8333333333333334\n",
      "\n",
      " The value of Recall 0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "print('\\n Accuracy of the classifier is',metrics.accuracy_score(ytest,predicted))\n",
    "print('\\n Confusion matrix')\n",
    "print(metrics.confusion_matrix(ytest,predicted))\n",
    "print('\\n The value of Precision', metrics.precision_score(ytest,predicted))\n",
    "print('\\n The value of Recall', metrics.recall_score(ytest,predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a9e9ee",
   "metadata": {},
   "source": [
    "# Method 2: Using only Nouns from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bdb40509",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Hasan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Library to identify the type of the word i.e [noun pronoun,verb....]\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fafe8d",
   "metadata": {},
   "source": [
    "# Preparing Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c40aa805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying tpe of each word in a doc\n",
    "nouns_xtrain_data={}\n",
    "for doc in xtrain_data:\n",
    "    nouns_xtrain_data[doc]=nltk.pos_tag(xtrain_data[doc])\n",
    "    \n",
    "# Extracting only Nouns from the doc, dropping all other type of words.\n",
    "for doc in xtrain_data:\n",
    "    nouns_xtrain_data[doc] = [word for word,pos in nouns_xtrain_data[doc] if pos == 'NN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "22902b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Index\n",
    "Noun_Training_Index = generate_index(nouns_xtrain_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3c3a998e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Nouns IDF-scores\n",
    "Noun_idf_scores = tf_idf_score(Noun_Training_Index, xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "55914702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'homepage': 1.0370376065048297, 'underconstruction': 2.9508514588885464}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: Noun_idf_scores[k] for k in list(Noun_idf_scores)[:2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1259b28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns_train_tf_idf_docs_dict = tf_idf_docs(Noun_Training_Index, Noun_idf_scores, xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b8f01432",
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns_train_tf_idf_doc_vectors = to_doc_vectors(nouns_train_tf_idf_docs_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e780024",
   "metadata": {},
   "source": [
    "# Preparing Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6e96c129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying tpe of each word in a doc\n",
    "nouns_xtest_data={}\n",
    "for doc in xtest_data:\n",
    "    nouns_xtest_data[doc]=nltk.pos_tag(xtest_data[doc])\n",
    "    \n",
    "# Extracting only Nouns from the doc, dropping all other type of words.\n",
    "for doc in xtest_data:\n",
    "    nouns_xtest_data[doc] = [word for word,pos in nouns_xtest_data[doc] if pos == 'NN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e18c8218",
   "metadata": {},
   "outputs": [],
   "source": [
    "Noun_Testing_Index = generate_index(nouns_xtest_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8720233d",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index={}\n",
    "i=0\n",
    "for word in Noun_idf_scores:\n",
    "    word_index[word]=i\n",
    "    i=i+1\n",
    "nouns_test_tf_idf_doc_vectors=[]\n",
    "for doc in nouns_xtest_data:\n",
    "    D=[0]*(len(Noun_idf_scores))\n",
    "    for word in nouns_xtest_data[doc]:\n",
    "        if word in Noun_Training_Index:\n",
    "            D[word_index[word]]=int(Noun_Testing_Index[word][test_doc_index[doc]])*float(Noun_idf_scores[word])\n",
    "    nouns_test_tf_idf_doc_vectors.append(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5776d1",
   "metadata": {},
   "source": [
    "# Fitting Same Model and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f7d30432",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB().fit(nouns_train_tf_idf_doc_vectors,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3c6eb7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = clf.predict(nouns_test_tf_idf_doc_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "40f44588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "45d6ecd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Accuracy of the classifier is 0.9113924050632911\n",
      "\n",
      " Confusion matrix\n",
      "[[114   9]\n",
      " [  5  30]]\n",
      "\n",
      " The value of Precision 0.7692307692307693\n",
      "\n",
      " The value of Recall 0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "print('\\n Accuracy of the classifier is',metrics.accuracy_score(ytest,predicted))\n",
    "print('\\n Confusion matrix')\n",
    "print(metrics.confusion_matrix(ytest,predicted))\n",
    "print('\\n The value of Precision', metrics.precision_score(ytest,predicted))\n",
    "print('\\n The value of Recall', metrics.recall_score(ytest,predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2a334d",
   "metadata": {},
   "source": [
    "# Method 3: Using only Top Frequent terms in each class\n",
    "Extracting top 50 nouns from each class which appears in most number of documents in each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "00e989c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words_in_class(data,class_name): \n",
    "    Index={}\n",
    "    for doc in data:\n",
    "        if doc in class_name:\n",
    "            for word in data[doc]:\n",
    "                if word not in stopword:\n",
    "                    if word not in Index:\n",
    "                        Index[word]=0\n",
    "                    Index[word]=Index[word]+1\n",
    "    return Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "245b993e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperating both classes documents to retrive most frequent words from each class\n",
    "course_class_train = [doc for doc,doc_label in zip(xtrain,ytrain) if doc_label==1]\n",
    "non_course_class_train = [doc for doc,doc_label in zip(xtrain,ytrain) if doc_label==0]\n",
    "# course_class_test = [doc for doc,doc_label in zip(xtest,ytest) if doc_label==1]\n",
    "# non_course_class_test = [doc for doc,doc_label in zip(xtest,ytest) if doc_label==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "148e0b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing duplicate words in a doc\n",
    "xtrain_data_unique={}\n",
    "for doc in xtrain_data:\n",
    "    xtrain_data_unique[doc] = list(set(xtrain_data[doc]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "003293dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting top words from course docs\n",
    "course_words = count_words_in_class(xtrain_data_unique,course_class_train)\n",
    "sorted_course_words = dict(sorted(course_words.items(), key=lambda item: item[1],reverse=True))\n",
    "top_50_course_words = {k: sorted_course_words[k] for k in list(sorted_course_words)[:100]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0b1d95c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting top words from non-course docs\n",
    "non_course_words = count_words_in_class(xtrain_data_unique,non_course_class_train)\n",
    "sorted_non_course_words = dict(sorted(non_course_words.items(), key=lambda item: item[1],reverse=True))\n",
    "top_50_non_course_words = {k: sorted_non_course_words[k] for k in list(sorted_non_course_words)[:100]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cc1878fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining both sets to form a corpus which will be our features\n",
    "unique_course_words = [word for word in top_50_course_words if word not in top_50_non_course_words]\n",
    "unique_non_course_words = [word for word in top_50_non_course_words if word not in top_50_course_words]\n",
    "all_words = unique_course_words + unique_non_course_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1afa0a67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total number of words(features)\n",
    "len(all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7039aed4",
   "metadata": {},
   "source": [
    "# Preparing Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "da1ef3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_noun_index(data): \n",
    "    Index={}\n",
    "    i=0\n",
    "    for doc in data:\n",
    "        for word in data[doc]:\n",
    "            if word in all_words:\n",
    "                if word not in Index:\n",
    "                    Index[word]=[]\n",
    "                    Index[word]=[0]*(len(data))\n",
    "                Index[word][i]=Index[word][i]+1\n",
    "        i=i+1\n",
    "    return Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "90bf1113",
   "metadata": {},
   "outputs": [],
   "source": [
    "Top_words_training_index = generate_noun_index(xtrain_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "04193360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Nouns IDF-scores\n",
    "Top_words_idf_scores = tf_idf_score(Top_words_training_index, xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "76f89c36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'homepage': 0.9966089494492215, 'updated': 0.843641489240678}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: Top_words_idf_scores[k] for k in list(Top_words_idf_scores)[:2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0e400612",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words_train_tf_idf_docs_dict = tf_idf_docs(Top_words_training_index, Top_words_idf_scores, xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "64a6e378",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words_train_tf_idf_doc_vectors = to_doc_vectors(top_words_train_tf_idf_docs_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abaa76c9",
   "metadata": {},
   "source": [
    "# Preparing Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1706975f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Top_words_testing_Index = generate_noun_index(xtest_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "35d7554a",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index={}\n",
    "i=0\n",
    "for word in Top_words_idf_scores:\n",
    "    word_index[word]=i\n",
    "    i=i+1\n",
    "top_words_test_tf_idf_doc_vectors=[]\n",
    "for doc in xtest_data:\n",
    "    D=[0]*(len(Top_words_idf_scores))\n",
    "    for word in xtest_data[doc]:\n",
    "        if word in Top_words_training_index:\n",
    "            D[word_index[word]]=int(Top_words_training_index[word][test_doc_index[doc]])*float(Top_words_idf_scores[word])\n",
    "    top_words_test_tf_idf_doc_vectors.append(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f4e392",
   "metadata": {},
   "source": [
    "# Fitting Same Model and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "87736999",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB().fit(top_words_train_tf_idf_doc_vectors,ytrain)\n",
    "predicted = clf.predict(top_words_test_tf_idf_doc_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6a7c2516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7c51298f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Accuracy of the classifier is 0.810126582278481\n",
      "\n",
      " Confusion matrix\n",
      "[[118   5]\n",
      " [ 25  10]]\n",
      "\n",
      " The value of Precision 0.6666666666666666\n",
      "\n",
      " The value of Recall 0.2857142857142857\n"
     ]
    }
   ],
   "source": [
    "print('\\n Accuracy of the classifier is',metrics.accuracy_score(ytest,predicted))\n",
    "print('\\n Confusion matrix')\n",
    "print(metrics.confusion_matrix(ytest,predicted))\n",
    "print('\\n The value of Precision', metrics.precision_score(ytest,predicted))\n",
    "print('\\n The value of Recall', metrics.recall_score(ytest,predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240d0679",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Using all the data as features used in method 1 seems to be the best approach with this dataset using Naive Bayes Algorithm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
